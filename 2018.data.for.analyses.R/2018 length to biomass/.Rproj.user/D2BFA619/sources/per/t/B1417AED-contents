#1.14.19 -- GCJ added this in to mess around wtih Casey's example
#associated with "mydata' tab

#Code for doing regression in R

#Clear the environment
rm(list=ls())

#Import the data. 
#We'll use our old friend, the SnailData as an example
mydata<-read.csv("Data/SnailData (1).csv")
mydata <- read.csv("Data/lb.1.14.19.csv")
mydata$lnlength<-log(mydata$Length)
mydata$lnweight<-log(mydata$Weight)
View(mydata)

#I find NA's hard to deal with some regression plots, so I'm just going to get rid
#of that NA right up front
#mydata<-mydata[-26,]
#We're going to ask whether the length of the snail determines its weight.
#So let's fit a model that describes that relationship

model1<-lm(Weight~Length, data=mydata, na.action="na.omit")
model3<-lm(lnweight~lnlength, data=mydata,na.action="na.omit")

#lm = linear model
#Weight is the dependent variable (y)
#Length is the independent variable (x)

#Before we look at the results, let's test the assumptions of the model
#We can call the residuals of the model as:
resid(model1) #better to name these as something
model1res<-resid(model1)
model3res<-resid(model2)
#Now we can test the normality of the residuals
library(car)
qqp(model1res, "norm")
qqp(model3res, "norm")

#We can also call the fitted y values as:
fitted(model1)
fitted(model3)

#To test for homogeneity of variance, we want to plot the fitted (predicted) values
#against the residuals
plot(model1res~fitted(model1))
plot(model3res~fitted(model2))
#This looks pretty good. We want to see no pattern. If we see a cone pattern, that is problematic

#A shortcut to doing all of the above.
plot(model1) #gives plots that show normality, homogeneity of variance, and potential outliers
plot(model3)

#Ok, if we're satisfied with our assumptions being met, then let's see how well our model fits the data
#To see the results of the model

summary(model1)
summary(model3)

#Ideally we would do this as a Model II regression, especially if we're interested in knowing
#the true estimate of the slope. Model II is appropriate because we have error in our measures
#of both x and y.
library(lmodel2)
model2<-lmodel2(Weight~Length, range.y="relative", range.x="relative", data=mydata, nperm=99)
model2
model2a<-lmodel2(lnweight~lnlength, range.y="interval", range.x="relative", data=mydata, nperm=99)
#Note: "relative" means the variable has a true zero at some point
#"interval" means the variables can include negative values
model2a

#how to make plot
#There are lots of options to add on here (see Using base Plot code)
#In the simplest form:
plot(Weight~Length, data=mydata, col="blue", ylab="Weight (mg)", xlab="Length (cm)")
abline(model1, col="blue") #adds fit from the model

plot(lnweight~lnlength, data=mydata, col="blue", ylab="Log Weight (g)", xlab="Log Length (cm)")
abline(model3, col="blue") #adds fit from the model

#want to add confidence intervals to the regression line?
prd<-predict(model1, interval="confidence")
#the above gives a table with predicted values and upper and lower confidence intervals
lines(mydata$Length, prd[,2], lty=2) #adds lower CL
lines(mydata$Length, prd[,3], lty=2) #adds upper CL

prd<-predict(model3, interval="confidence")
#the above gives a table with predicted values and upper and lower confidence intervals
lines(mydata$lnlength, prd[,2], lty=2) #adds lower CL
lines(mydata$lnlength, prd[,3], lty=2) #adds upper CL

#I didn't like these funky lines, which seems to be fixed by making
#a new x variable that is more finely grained. So try this
newx<-seq(min(mydata$Length), max(mydata$Length), 0.1)
newpredict<-predict(model1, newdata=data.frame(Length=newx), interval="confidence")
plot(Weight~Length, data=mydata, col="blue", ylab="Weight (mg)", xlab="Length (cm)")
abline(model1, col="blue")
lines(newx, newpredict[,2], lty=2)
lines(newx, newpredict[,3], lty=2)

#myturn

newx<-seq(min(mydata$lnlength), max(mydata$lnlength), 0.1)
newpredict<-predict(model3, newdata=data.frame(lnlength=newx), interval="confidence")
plot(lnweight~lnlength, data=mydata, col="blue", ylab="Log Weight (g)", xlab="Log Length (mm)")
abline(model3, col="blue")
lines(newx, newpredict[,2], lty=2)
lines(newx, newpredict[,3], lty=2)


#Or you could also do this with ggplot
#If you find that far too clunky, then use ggplot:
library(ggplot2)
ggplot(mydata, aes(x=Length, y=Weight))+
  theme(panel.grid.major=element_blank(), panel.grid.minor=element_blank()) +
  geom_point(shape=1) + 
  guides(fill=FALSE) + 
  ylab("Weight (mg)") +
  xlab("Length (cm)") +
  geom_smooth(method="lm")

#myturn

library(ggplot2)
p<-ggplot(mydata, aes(x=lnlength, y=lnweight))+
  theme(panel.grid.major=element_blank(), panel.grid.minor=element_blank()) +
  geom_point(shape=1) + 
  guides(fill=FALSE) + 
  ylab("Log Weight (g)") +
  xlab("Log Length (mm)") +
  geom_smooth(method="lm")

#trying to figure out how to display the equaiton of the line on the plot...
#not 100% necessary yet, but would be nice
#################ignore below here for now#########
#pulled info from here: 
# https://stackoverflow.com/questions/7549694/adding-regression-line-equation-and-r2-on-graph
p + geom_text(x = 2.5, y = 0, label = lm_eqn(df))

lm_eqn <- function(df){
  m <- lm(y ~ x, df);
  eq <- substitute(italic(y) == a + b %.% italic(x)*","~~italic(r)^2~"="~r2, 
                   list(a = format(coef(m)[1], digits = 2), 
                        b = format(coef(m)[2], digits = 2), 
                        r2 = format(summary(m)$r.squared, digits = 3)))
  as.character(as.expression(eq));                 
}

###########testing equation of the line in R###################
#equation = y(log weight)= 2.875048x(log length) â€“ 10.52898

mydata$test.weight<-2.875048*(mydata$lnlength)-10.52898
View(mydata)
#worked fairly well

#now want to see if I can take the anitlog to backcalculate the weight values
mydata$antilog.weight<-10^(mydata$test.weight)
View(mydata)
###grrr....can't figure out what I'm doing wrong here,
#but I think it might have something to do with the fact that
#I'm antilogging one part of the entire equation (just weight?)
##Antilog values that I'm calculating are off by about 2 orders of magnitude